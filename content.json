{"meta":{"title":"데이터 바다의 요요누","subtitle":"","description":"","author":"요요누","url":"http://ioio-noo.github.io","root":"/"},"pages":[{"title":"all-categories","date":"2020-10-31T19:42:51.364Z","updated":"2020-10-31T19:42:51.364Z","comments":false,"path":"all-categories/index.html","permalink":"http://ioio-noo.github.io/all-categories/index.html","excerpt":"","text":""},{"title":"all-tags","date":"2020-10-31T19:43:00.626Z","updated":"2020-10-31T19:43:00.626Z","comments":false,"path":"all-tags/index.html","permalink":"http://ioio-noo.github.io/all-tags/index.html","excerpt":"","text":""}],"posts":[{"title":"k-Means","slug":"006-k-means","date":"2020-11-01T02:52:39.000Z","updated":"2020-11-01T03:00:46.981Z","comments":true,"path":"2020/11/01/006-k-means/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/006-k-means/","excerpt":"","text":"k-Means Clustering각 클러스터와 거리 차이의 분산을 최소화하는 방식이다. EM식 클러스터링과 유사한 면이 있다. 알고리즘 개요 Input: k = Number of clusters to be used x, y = two numeric vectors identical in lengths x와 y의 random cluster centers 생성:x.centers ~ Unif( min(x), max(x) )y.centers ~ Unif( min(y), max(y) ) i = 1, 2, … , length(x)일 때x[i]와 x.centers, y[i]와 y.centers 간의 거리를 통해 유클리드 거리 측정개중 가장 거리가 가깝게 나타나는 x.center, y.center 쌍에 해당하는 cluster를 해당 점에 매칭 subset(x, cluster)의 평균 계산 후 이 값과 기존의 x.center, y.center이 동일하면 loop 중단 빠르다. k-Median, k-Means++, k-Medoid 등의 변형이 있다. Elbow Method: within-cluster sum of squares를 line chart로 나타내어 눈에 띄게 꺾이는 부분을 클러스터 개수로 설정하는 경우가 많다. x, y에 각각 N(10, 4), N(-10, 2)를 따르는 50개의 난수를 부여하고, 이를 클러스터링하였다. k값에 따른 클러스터의 분포는 다음과 같이 나타난다. Partitioning Around Medoids (PAM) k-Medoid 클러스터링 중 가장 널리 쓰이는 알고리즘이다. 노이즈와 이상치에 보다 Robust하다. 해석이 용이하다. Greedy Search: 최적치에 도달하지 못할 수 있다. 그래도 Exhaustive Search보단 훨씬 빠르다. 알고리즘 개요 Initialize: greedily select k of the n data points as the medoids to minimize the cost Associate each data point to the closest medoid. While the cost of the configuration decreases: For each medoid m, and for each non-medoid data point o: Consider the swap of m and o, and compute the cost change If the cost change is the current best, remember this m and o combination Perform the best swap of $m_{best}$and $o_{best}$, if it decreases the cost function. Otherwise, the algorithm terminates. 대푯값(medoid)을 도출하는 데에는 Gower’s Distance를 활용할 수 있다.","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Optimization","slug":"005-optimization","date":"2020-11-01T02:50:59.000Z","updated":"2020-11-01T02:51:52.440Z","comments":true,"path":"2020/11/01/005-optimization/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/005-optimization/","excerpt":"","text":"local maxima/minima를 찾는 고전적 알고리즘 세 가지를 알아보자. Bisection Method도함수가 0이 되는 지점을 찾기 위해 대상 구간을 반으로 자른다. 잘린 경계값에서의 도함수 값을 곱해 음수가 되는 부분을 찾는다. 음수인 부분을 또 자른다. 이렇게 반복해서 반절(半折)하다 보면 0에 수렴하는 지점을 찾을 수 있다. 123456789101112131415161718192021bisec.ftn = function(g,a,b)&#123; iter = 0 threshold = 10^(-9) x.new = (a+b)/2 err = 1 while(iter&lt; 3000 &amp; err&gt;=threshold)&#123; z.a = genD(g,a) z.x = genD(g,x.new) if(z.a$D[1] * z.x$D[1] &lt; 0) b = x.new else a = x.new old.x = x.new x.new = (a+b)/2 err = abs(old.x - x.new) iter = iter + 1 &#125; return(list(MLE = x.new, iterations = iter))&#125; Newton’s Method (Newton-Rhapson)테일러 전개된 수식을 통해 $x_{t+1}$ 을 업데이트한다. 직관적이고 간편하지만 초기값을 잘못 설정하면 수렴이 불가능할 수 있으며, 이차 도함수를 모르면 사용할 수 없다. 1234567891011121314151617newton.ftn = function(g, theta)&#123; threshold = 10^(-9) err = 1 theta_0 = theta iter = 1 while(err &gt;= threshold &amp; iter &lt; 3000)&#123; z = genD(g, theta_0) theta.new = theta_0 - z$D[1]/z$D[2] err = abs(theta.new - theta_0) theta_0 = theta.new iter = iter + 1 &#125; return(list(MLE = theta.new, iterations = iter))&#125; Secant MethodNewton’s Method의 파생 알고리즘이다. 두 개의 초기값을 설정해 계산한다. 발산 문제에서 자유로우며, 이차 도함수를 알지 못해도 적용할 수 있다. 1234567891011121314151617181920secant.ftn = function(g,a,b)&#123; iter = 1 threshold = 10^(-9) err = 1 x0 = a x1 = b while(err &gt;= threshold &amp; iter &lt; 3000)&#123; z0 = genD(g, x0) z1 = genD(g, x1) x.new = x1 - z1$D[1]*(x1 - x0) / (z1$D[1] - z0$D[1]) err = abs(x.new - x1) x0 = x1 x1 = x.new iter = iter + 1 &#125; return(list(MLE = x.new, iterations = iter))&#125; $Cauchy(\\theta, 1)$ 분포에서 추출한 샘플을 통해 MLE를 구한다면, 각 method는 어떻게 작동할까? 위와 같은 로그우도함수에 대해 각 방식은 아래와 같은 결과를 보였다. Bisection Starting Points Estimated Iterations (-1, 1) -0.1922866 34 (-100, 100) -0.1922866 40 (-10^6, -10^6) -0.1922866 54 (-1, 3) -0.1922866 35 (30, 40) 40 36 Global Maximum이 없는 구간에서 시작하면 MLE를 찾을 수 없다. Newton-Rhapson Starting Points Estimated Iterations -11 - - -1 - 0.1922866 7 0 - 0.1922866 5 1.5 1.713587 6 4 2.817472 7 4.7 - 0.1922866 7 7 41.04085 11 8 - - 38 42.79538 7 5.689 54.87662 10 Secant Starting Points Estimated Iterations (-2, -1) -0.1922866 8 (-3, 3) 2.817472 9 (-10^6, -10^6) - - (-1, 3) 1.713587 12 (30, 40) 42.79538 13 좀 더 복잡한 분포를 살펴보자. f (x) = \\frac{[1 − cos(x − \\theta)]}{2π}0 ≤ x ≤ 2π, where θ is a parameter between −π and π 위와 같은 로그우도함수에 대해 Newton-Rhapson 방식을 적용하자. 취약점이 보일 것이다. $\\theta$ 의 method-of-moments estimator은 $\\hat \\theta = arcsin(\\pi - \\hat \\mu) = -0.05844$ 이므로 이 부근에서 시작한 경우와 랜덤한 두 점 (-2.7과 2.7)에서 시작한 경우를 비교해 보았다. Newton-Rhapson Starting Points Estimated Iterations -0.0584 -0.01197 7 -2.7 -2.6667 6 2.7 2.8731 7 서로 판이하게 다른 값이 도출된다. MoM 추정치에서 시작한 값만이 정답에 수렴하고 있다. 동일한 크기로 슬라이스한 200개의 구간에서 각각 결과를 구해 보았다. 앞의 19개에서는 이러한 아웃풋이 나오는데, Group 11 (-0.81681, 0.52027) 에서 또 정답이 보인다. Group Lower Upper Estimated 1 -3.14159 -2.82743 -3.09309 2 -2.79602 -2.7646 -2.78617 3 -2.73319 -2.60752 -2.6667 4 -2.57611 -2.41903 -2.50761 5 -2.38761 -2.38761 -2.3882 6 -2.35619 -2.26195 -2.29726 7 -2.23053 -2.23053 -2.23217 8 -2.19911 -1.47655 -1.65828 9 -1.44513 -1.44513 -1.44748 10 -1.41372 -0.84823 -0.95334 11 -0.81681 0.502655 -0.01197 12 0.534071 1.947787 0.7906 13 1.979203 2.199115 2.00364 14 2.230531 2.261947 2.23622 15 2.293363 2.450442 2.36072 16 2.481858 2.481858 2.47537 17 2.513274 2.513274 2.51359 18 2.54469 2.984513 2.87309 19 3.0159289 3.1101767 3.19009 서로 아주 가까운 두 값을 시작값으로 상정한다면 이러한 결과를 얻게 된다. Lower Upper Estimated -1.41372 -0.82377 -0.95334 -0.82209 0.403176 -0.01197 0.404857 0.404857 8.28683 0.406538 0.492256 -0.01197 0.493937 0.493937 0.7906 0.495618 0.519148 -0.01197 0.520829 1.946107 0.7906 요약 Bisection Method는 일정 구간의 경계값에서 미분값을 곱하여 음수가 되는 곳을 따라 진행하므로, 구간의 길이가 길수록 연산 시간도 길어지게 되며, 연산의 효율성이 떨어진다. 대신 발산 문제에서 자유로운 편이다. Newton’s Method는 초기값에 영향을 크게 받으며, 적절치 못한 값으로 연산을 시작하게 된다면 정확하지 않은 결과를 얻게 될 가능성이 크다. Secant Method는 이차 도함수를 얻기 어려운 경우, Bisection Method보다 빠르게 값을 도출할 수 있는 방법이라 할 수 있다.","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"차원 축소","slug":"008-t-SNE","date":"2020-11-01T02:18:18.000Z","updated":"2020-11-01T02:30:23.782Z","comments":true,"path":"2020/11/01/008-t-SNE/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/008-t-SNE/","excerpt":"","text":"PCAt-SNE 데이터의 공분산을 분석하여 선형변환 행렬(linear transformation matriix)을 얻는다. PCA는 선형변환을 이용하기 때문에 비선형 특성을 가진 데이터의 특성을 잘 추출하지 못한다. 기존의 방식대로 KL-divergence로 similarity를 계산하는 것은 symetric하지 않다. Sillhouette Width 클러스터링 품질 점수 $x_i$의 실루엣 계수: $a(i)$: 클러스터 내 데이터 응집도 (cohesion) $b(i)$: 클러스터 간 분리도 (separation) 1에 가까울수록 좋다","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Stepwise Selection과 Regsubset","slug":"007-step-regsubset","date":"2020-11-01T01:08:18.000Z","updated":"2020-11-01T02:16:54.275Z","comments":true,"path":"2020/11/01/007-step-regsubset/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/007-step-regsubset/","excerpt":"","text":"변수 선택하기 - Stepwise Selection, regsubsetsbaseball 데이터에는 야수 337명에 대한 27가지의 스탯과 연봉이 나타나 있다. 스탯은 1991년도 기준이고, 연봉은 1992년도에 책정된 값이다. (데이터) 스탯의 변수명 average OBP runs hits doubles triples homeruns 타율 출루율 득점 안타 2루타 3루타 홈런 SBs RBIs walks SOs errors freeagent arbitration 도루 타점 볼넷 삼진 실책 FA 연봉조정 runs/SOs, hits/SOs , RBIs/SOs , walks/SOs OBP/errors, runs/errors , hits/errors, homeruns/errors SOs*errors, SBs*OBP, SBs*runs, SBs*hits 연봉을 반응변수로 삼아 회귀 모형을 만들어 보자. 이를 Full Model, 또는 Saturated Model이라 한다. 다중 회귀 모형에는 관행적으로 변수 선택을 진행한다. 줄여진 모형은 Reduced Model이라 한다. AIC를 기준으로 변수를 줄이는 방법을 두 가지 살펴보겠다. Stepwise SelectionStepwise Selection 알고리즘은 변수를 하나씩 제거하거나 추가하면서 각 경우의 AIC를 비교하고, 개중 가장 낮은 값을 갖는 모형을 찾는다. Backward Selection: 모든 변수를 가진 saturated model에서 변수를 하나씩 소거해 나간다. 제거 후에 AIC가 작아졌다면 제거된 모형을 채택하고 더 이상 AIC가 나아지지 않을 때까지 같은 작업을 반복한다. Forward Selection: 반대로 상수만 지닌 reduced model에서 변수를 하나씩 추가하는 방식이다. “Both”: 두 가지를 절충하여 추가와 제거를 반복하고자 하는 경우, (R 기준) both 인자를 넣는다. 각 방식을 적용한 결과는 다음과 같이 나타났다. 채택 변수 수 최종 AIC Backward 12 -418.9421 Forward 27 -396.706 Both 12 -418.9421 Forward는 전체 설명변수를 모두 가져왔다. Both와 Backward에서는 동일한 모형이 채택되었다. 이 때 선택된 변수는 다음과 같다: 타율(-), 득점(+), 삼루타(-), 타점(+), 삼진(-), FA(+), 연봉조정(+), 득점/삼진(-), 안타/삼진(+), 삼진x실책(-), 도루x출루율(+), 도루x득점(-). regsubsetsregsubsets 알고리즘은 고정된 변수 개수에 맞추어 최적 모형을 도출한다. 12345678910111213141516171819regsub.AIC.ftn = function(j)&#123; model.row = regsub.baseball$which[j,] model.exp = &#x27;lm( log(salary) ~ &#x27; for (i in 2:28)&#123; if (model.row[i] == TRUE)&#123; var.name = names(model.row)[i] model.exp = paste(model.exp, &quot; + &quot;, var.name) &#125; &#125; model.exp = paste(model.exp,&quot;, data = df.baseball)&quot;) result = eval(parse(text = model.exp)) if(j &lt; 14)&#123; varnames = paste(names(model.row)[model.row], collapse = &#x27; &#x27;) &#125; else varnames = paste(&#x27;-&#x27;, names(model.row)[!model.row], collapse = &#x27; &#x27;) return(c(AIC = extractAIC(result)[2], var = varnames))&#125; p = 1에서 27까지의 최적 Subsets는 아래의 표에 나타내었다. 개중 AIC가 가장 작은 모형은 p = 12에서 얻을 수 있었다. p AIC Features Selected 1 -86.20355 hits 2 -236.91899 freeagent arbitration 3 -382.08585 rbis freeagent arbitration 4 -397.06152 hits rbis freeagent arbitration 5 -406.00412 hits rbis freeagent arbitration soserrors 6 -410.43729 runs rbis sos freeagent arbitration runsperso 7 -413.72379 runs rbis sos freeagent arbitration runsperso hitsperso 8 -416.30347 runs rbis sos freeagent arbitration obppererror runspererror hitspererror 9 -416.42493 runs rbis sos freeagent arbitration obppererror runspererror hitspererror soserrors 10 -417.47395 runs rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 11 -418.45106 average runs rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 12 -418.94721 obp runs triples rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 13 -418.09221 average runs triples rbis sos freeagent arbitration runsperso hitsperso hrsperso soserrors sbsobp sbsruns 14 -417.20098 - average - hits - doubles - homeruns - sbs - errors - rbisperso - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 15 -416.22752 - average - hits - doubles - homeruns - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 16 -415.45316 - average - doubles - walks - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 17 -414.01172 - average - doubles - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 18 -413.16254 - average - doubles - walks - sbs - walksperso - runspererror - hrspererror - soserrors - sbshits 19 -411.53320 - average - doubles - sbs - walksperso - runspererror - hrspererror - soserrors - sbshits 20 -410.00084 - obp - doubles - sbs - runspererror - hrspererror - soserrors - sbshits 21 -408.22846 - obp - doubles - sbs - errors - hrspererror - sbshits 22 -406.42133 - obp - sbs - errors - hrspererror - sbshits 23 -404.56810 - obp - sbs - errors - sbshits 24 -402.63184 - obp - errors - sbshits 25 -400.66945 - errors - sbshits 26 -398.69257 - sbshits 27 -396.70603 - 최적의 모형인 p = 12에서 선택된 변수는 출루율(-), 득점(+), 삼루타(-), 타점(+), FA(+), 연봉조정(+), 득점삼진비(-), 안타삼진비(+), 도루x실책(-), 도루x출루율(+), 도루x득점(-)이다. 양쪽 결과가 비슷하게 나타났다. 문장으로 풀어 쓴다면 안타가 많고 타율이 좋은 타자는 오히려 낮은 연봉을 받는다고 해석할 수 있다. 이는 높은 연봉이 책정되는 홈런 타자들에게 영향을 받은 결과로 보인다. (3루타의 경우, 장타력뿐만 아니라 상당한 주력이 함께 뒷받침되어야 하는 데다 부상의 위험이 따르므로 대부분의 거포형 선수와 베테랑 선수들은 2루에서 멈추게 된다.) 큰 폭으로 연봉이 상승하게 되는 FA 전환 여부 또한 모형에 주는 영향이 크다. 연봉 조정 역시 같은 맥락에서 해석할 수 있을 것이다. * ScopeR에서 step함수를 사용한다면 변수의 범위를 지정할 수 있다. 123step(model, direction = [&#x27;forward&#x27;,&#x27;backward&#x27;], scope = list(upper = y ~ ., lower = y ~ x1 + x2 + x3 + x4 + x5))","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"004-lift","slug":"004-lift","date":"2020-11-01T00:04:48.000Z","updated":"2020-11-01T00:04:48.556Z","comments":true,"path":"2020/11/01/004-lift/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/004-lift/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Reducting Feature Dimensions","slug":"003-curse-of-dimensionality","date":"2020-10-31T23:01:12.000Z","updated":"2020-11-01T02:56:51.600Z","comments":true,"path":"2020/11/01/003-curse-of-dimensionality/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/003-curse-of-dimensionality/","excerpt":"","text":"차원 축소하기Curse of Dimensionality: 차원의 저주간단히 말해 고차원 데이터를 경계해야 한다는 뜻이다. 고차원 공간은 엄청나게 넓기 때문에 공간 속에 있는 데이터 포인트는 매우 외로운 나날을 보내게 된다. (near neighbor의 수가 적다고 표현한다.) n개의 points를 갖는 random sample에 대해 kernel density estimator을 정의하자. (이 때, 해당 데이터는 p차원의 표준정규분포를 따른다고 가정한다.) 굳이 알 필요는 없지만, 진짜 mode에 대한 다변량 density estomator의 퀄리티를 판단하는 기준은 ORRMSE(Optimal Relative Root MSE)이다. ORRMSE의 식은 다음과 같다. ORRMSE(p,n) = \\frac{\\sqrt{min_h[MSE_h(\\hat f(0))]}}{f(0)}우리의 목표는 ORRMSE를 작게 유지하는 것이다. 차원 p가 올라가면 적정 샘플 사이즈 n은 그보다 훨씬 큰 폭으로 증가한다. Computational Statistics (Givens and Hoeting, Wiley) 에 제시된 예시에 대한 결과표를 보자. p n 1 30 2 180 3 806 5 17,400 10 112,000,000 15 2,190,000,000,000 30 806,000,000,000,000,000,000,000,000 변수가 수백 개인 데이터도 드물지 않은데, 그 때마다 어마어마한 양의 샘플을 준비하는 것은 현실적으로 불가능하다. 그렇다면 해결책은 하나 뿐이다. 차원을 줄이는 것이다. LASSO, Ridge, mRMR, SVM-REF t-SNE PCA SVD LDA 변수 선택하기 - Stepwise Selection, regsubsetsbaseball 데이터에는 야수 337명에 대한 27가지의 스탯과 연봉이 나타나 있다. 스탯은 1991년도 기준이고, 연봉은 1992년도에 책정된 값이다. (데이터) 스탯의 변수명 average OBP runs hits doubles triples homeruns 타율 출루율 득점 안타 2루타 3루타 홈런 SBs RBIs walks SOs errors freeagent arbitration 도루 타점 볼넷 삼진 실책 FA 연봉조정 runs/SOs, hits/SOs , RBIs/SOs , walks/SOs OBP/errors, runs/errors , hits/errors, homeruns/errors SOs*errors, SBs*OBP, SBs*runs, SBs*hits 연봉을 반응변수로 삼아 회귀 모형을 만들어 보자. 이를 Full Model, 또는 Saturated Model이라 한다. 다중 회귀 모형에는 관행적으로 변수 선택을 진행한다. 줄여진 모형은 Reduced Model이라 한다. AIC를 기준으로 변수를 줄이는 방법을 두 가지 살펴보겠다. Stepwise SelectionStepwise Selection 알고리즘은 변수를 하나씩 제거하거나 추가하면서 각 경우의 AIC를 비교하고, 개중 가장 낮은 값을 갖는 모형을 찾는다. Backward Selection: 모든 변수를 가진 saturated model에서 변수를 하나씩 소거해 나간다. 제거 후에 AIC가 작아졌다면 제거된 모형을 채택하고 더 이상 AIC가 나아지지 않을 때까지 같은 작업을 반복한다. Forward Selection: 반대로 상수만 지닌 reduced model에서 변수를 하나씩 추가하는 방식이다. “Both”: 두 가지를 절충하여 추가와 제거를 반복하고자 하는 경우, (R 기준) both 인자를 넣는다. 각 방식을 적용한 결과는 다음과 같이 나타났다. 채택 변수 수 최종 AIC Backward 12 -418.9421 Forward 27 -396.706 Both 12 -418.9421 Forward는 전체 설명변수를 모두 가져왔다. Both와 Backward에서는 동일한 모형이 채택되었다. 이 때 선택된 변수는 다음과 같다: 타율(-), 득점(+), 삼루타(-), 타점(+), 삼진(-), FA(+), 연봉조정(+), 득점/삼진(-), 안타/삼진(+), 삼진x실책(-), 도루x출루율(+), 도루x득점(-). regsubsetsregsubsets 알고리즘은 고정된 변수 개수에 맞추어 최적 모형을 도출한다. 12345678910111213141516171819regsub.AIC.ftn = function(j)&#123; model.row = regsub.baseball$which[j,] model.exp = &#x27;lm( log(salary) ~ &#x27; for (i in 2:28)&#123; if (model.row[i] == TRUE)&#123; var.name = names(model.row)[i] model.exp = paste(model.exp, &quot; + &quot;, var.name) &#125; &#125; model.exp = paste(model.exp,&quot;, data = df.baseball)&quot;) result = eval(parse(text = model.exp)) if(j &lt; 14)&#123; varnames = paste(names(model.row)[model.row], collapse = &#x27; &#x27;) &#125; else varnames = paste(&#x27;-&#x27;, names(model.row)[!model.row], collapse = &#x27; &#x27;) return(c(AIC = extractAIC(result)[2], var = varnames))&#125; p = 1에서 27까지의 최적 Subsets는 아래의 표에 나타내었다. 개중 AIC가 가장 작은 모형은 p = 12에서 얻을 수 있었다. p AIC Features Selected 1 -86.20355 hits 2 -236.91899 freeagent arbitration 3 -382.08585 rbis freeagent arbitration 4 -397.06152 hits rbis freeagent arbitration 5 -406.00412 hits rbis freeagent arbitration soserrors 6 -410.43729 runs rbis sos freeagent arbitration runsperso 7 -413.72379 runs rbis sos freeagent arbitration runsperso hitsperso 8 -416.30347 runs rbis sos freeagent arbitration obppererror runspererror hitspererror 9 -416.42493 runs rbis sos freeagent arbitration obppererror runspererror hitspererror soserrors 10 -417.47395 runs rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 11 -418.45106 average runs rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 12 -418.94721 obp runs triples rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 13 -418.09221 average runs triples rbis sos freeagent arbitration runsperso hitsperso hrsperso soserrors sbsobp sbsruns 14 -417.20098 - average - hits - doubles - homeruns - sbs - errors - rbisperso - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 15 -416.22752 - average - hits - doubles - homeruns - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 16 -415.45316 - average - doubles - walks - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 17 -414.01172 - average - doubles - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 18 -413.16254 - average - doubles - walks - sbs - walksperso - runspererror - hrspererror - soserrors - sbshits 19 -411.53320 - average - doubles - sbs - walksperso - runspererror - hrspererror - soserrors - sbshits 20 -410.00084 - obp - doubles - sbs - runspererror - hrspererror - soserrors - sbshits 21 -408.22846 - obp - doubles - sbs - errors - hrspererror - sbshits 22 -406.42133 - obp - sbs - errors - hrspererror - sbshits 23 -404.56810 - obp - sbs - errors - sbshits 24 -402.63184 - obp - errors - sbshits 25 -400.66945 - errors - sbshits 26 -398.69257 - sbshits 27 -396.70603 - 최적의 모형인 p = 12에서 선택된 변수는 출루율(-), 득점(+), 삼루타(-), 타점(+), FA(+), 연봉조정(+), 득점삼진비(-), 안타삼진비(+), 도루x실책(-), 도루x출루율(+), 도루x득점(-)이다. 양쪽 결과가 비슷하게 나타났다. 문장으로 풀어 쓴다면 안타가 많고 타율이 좋은 타자는 오히려 낮은 연봉을 받는다고 해석할 수 있다. 이는 높은 연봉이 책정되는 홈런 타자들에게 영향을 받은 결과로 보인다. (3루타의 경우, 장타력뿐만 아니라 상당한 주력이 함께 뒷받침되어야 하는 데다 부상의 위험이 따르므로 대부분의 거포형 선수와 베테랑 선수들은 2루에서 멈추게 된다.) 큰 폭으로 연봉이 상승하게 되는 FA 전환 여부 또한 모형에 주는 영향이 크다. 연봉 조정 역시 같은 맥락에서 해석할 수 있을 것이다. * ScopeR에서 step함수를 사용한다면 변수의 범위를 지정할 수 있다. 123step(model, direction = [&#x27;forward&#x27;,&#x27;backward&#x27;], scope = list(upper = y ~ ., lower = y ~ x1 + x2 + x3 + x4 + x5)) 변수 선택하기 -","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Collaborative Filtering","slug":"002-CF","date":"2020-10-31T22:57:54.000Z","updated":"2020-11-01T03:01:04.952Z","comments":true,"path":"2020/11/01/002-CF/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/002-CF/","excerpt":"","text":"협업 필터링User Based CF유저 기반 CF는 사용자 간 유사도를 기준으로 삼는다. Item Based CF","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Eigenvalue와 Eigenvectors","slug":"001-eigen","date":"2020-10-31T17:57:25.000Z","updated":"2020-10-31T22:04:48.000Z","comments":true,"path":"2020/11/01/001-eigen/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/001-eigen/","excerpt":"","text":"Eigenvalue, Eigenvectors 위와 같이, 임의의 정방행렬 A에 대해 AB = λB인 (단, B는 0이 아닌 벡터) B를 eigenvector, 상수 λ를 eigenvalue 라고 한다. Eigen Decomposition Av = \\lambda v \\\\ AQ = Q\\Lambda \\\\ A = Q\\Lambda Q^{-1}","categories":[],"tags":[{"name":"Eigenvalues, Eigenvectors, SVD","slug":"Eigenvalues-Eigenvectors-SVD","permalink":"http://ioio-noo.github.io/tags/Eigenvalues-Eigenvectors-SVD/"}]}],"categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"},{"name":"Eigenvalues, Eigenvectors, SVD","slug":"Eigenvalues-Eigenvectors-SVD","permalink":"http://ioio-noo.github.io/tags/Eigenvalues-Eigenvectors-SVD/"}]}