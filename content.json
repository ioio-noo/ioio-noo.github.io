{"meta":{"title":"데이터 바다의 요요누","subtitle":"","description":"","author":"요요누","url":"http://ioio-noo.github.io","root":"/"},"pages":[{"title":"all-archives","date":"2020-11-02T00:25:58.765Z","updated":"2020-11-02T00:25:58.765Z","comments":false,"path":"all-archives/index.html","permalink":"http://ioio-noo.github.io/all-archives/index.html","excerpt":"","text":""},{"title":"all-categories","date":"2020-10-31T19:42:51.364Z","updated":"2020-10-31T19:42:51.364Z","comments":false,"path":"all-categories/index.html","permalink":"http://ioio-noo.github.io/all-categories/index.html","excerpt":"","text":""},{"title":"all-tags","date":"2020-10-31T19:43:00.626Z","updated":"2020-10-31T19:43:00.626Z","comments":false,"path":"all-tags/index.html","permalink":"http://ioio-noo.github.io/all-tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Binary Classifier 평가하기","slug":"010-bc-assessments","date":"2020-11-02T00:06:30.000Z","updated":"2020-11-02T00:10:43.667Z","comments":true,"path":"2020/11/02/010-bc-assessments/","link":"","permalink":"http://ioio-noo.github.io/2020/11/02/010-bc-assessments/","excerpt":"","text":"이진 분류기(Binary Classifier)의 성능을 측정하는 지표들은 다음과 같다. cancer test 예시를 떠올리며 보도록 하자. 참고 decision boundary = cut-off value = threshold. 모두 같은 말이다. ROC-AUC수신자 조작 특성 곡선, receiver operating characteristics curve 아래 면적이다. [0.5, 1.0] 범위 내에서 나타난다. 진짜 양성을 모두 잡아내고 위양성도 없어야 1이 될 수 있다. 진양성률, True Positive Rate = sensitivity 위양성률, False Positive Rate = 1 - specificity 모형의 decision boundary가 이동하는 모양을 TPR-FPR 평면에 나타내면 y = x와 y = 1 사이에 위치하는 곡선이 만들어진다. 이 곡선이 ROC 커브고, 곡선 하단의 넓이가 ROC-AUC다. Accuracy정답률만 보고 싶다면 써도 좋지만, 그런 경우는 극히 드물다. Precision/Recall precision = \\frac{True\\,Positives}{TP +FP} = \\frac{양성으로\\,분류된\\,양성\\,수}{양성으로\\,분류된\\,케이스\\,수} recall = \\frac{True\\,Positives}{TP+FN} = \\frac{양성으로\\,분류된\\,양성\\,수}{양성\\,수}위음성만 치명적이라면 recall을, 위양성만 치명적이라면 precision을 각각 단독 지표로 삼을 수도 있다. 오분류율이 극히 낮지 않은 이상 두 지수는 trade-off 관계에 있는 경우가 많으며 이들을 엮어 PR-AUC를 측정하기도 한다. ROC-AUC만큼 로버스트하진 못하다. F1 Score F_1 = \\frac{2\\times precision\\times recall}{precision + recall}Precision과 recall의 조화평균. 두 수치를 모두 고려한 지표이다. 1에 가까울수록 좋다. 불균형 데이터에서도, 멀티클래스 데이터에서도, 불균형 멀티클래스 데이터에서도 쓸 수 있다.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[]},{"title":"Zero Inflated 데이터 이진분류","slug":"009-zero-inflated-binary","date":"2020-11-02T00:06:12.000Z","updated":"2020-11-02T00:22:40.551Z","comments":true,"path":"2020/11/02/009-zero-inflated-binary/","link":"","permalink":"http://ioio-noo.github.io/2020/11/02/009-zero-inflated-binary/","excerpt":"","text":"이 포스트는 석사 3학기 시절에 진행한 Macro 계정 감지 프로젝트를 떠올리며 작성 중이다. 초단위 위치 로그데이터 + IP에, 조사 기간 동안 몇 번을 앉고 뛰고 템줍을 했는지, 길드전엔 얼마나 나갔는지, 인간관계는 어땠는지 등등이 우르르 들어있는 어마어마한 데이터였다. 지금 와서는 실시간 Anomaly Detection 쪽으로 알고리즘을 짰어도 좋았겠다는 생각이 들지만… 일단 정성껏 Binary Classification을 해 봤다는 데에 의의를 두고 있다. y = [Macro, Human]인 Binary 반응변수를 상정했다. 전형적인 영과잉(Zero-Inflated) 데이터이다. 개인적으로 매크로 하면 필드에 몰려다니는 작업장산 외계어 매크로군단부터 생각났는데 생각보다 매크로 비율이 낮았다 (…). 좀 다른 이야기지만 EDA에서 좀 주의 깊게 봤더니 매크로 하나당 플레이타임이 어마무시해서 그렇게 느껴진 것 같았다. 일당백이었던 모양이다.. 영과잉 이진 데이터는 핸들링이 까다롭다. 과적합에 이만큼 취약한 데이터도 드물다. (이진 데이터만 그런 건 아니다. Count 데이터는 푸아송이나 음이항 분포를 전제하는 경우가 많으며 이 경우에도 0이 나올 확률은 실제보다 작게 예측되곤 한다.) 적절한 전처리가 반드시 수반되어야 하며, 데이터 특성에 맞는 분류 알고리즘을 적절히 골라 주는 것이 좋다. 전처리Random Over Sampling Examples 로 Training Set 를 구성했다. ROSE Random Over Sampling Examples 영과잉 이진 데이터 예측력 개선 학습에 사용될 데이터를 반복적으로 신규 합성생성(Synthetic Generation) 범주 내 관측값 하나를 훈련셋에서 추출해, 그 주변(neighborhood)에서 학습사례를 만드는 식이다. Smoothed Bootstrap이 활용된다. Smoothed Bootstrap: 재표본추출 관측치에 정규 white noise 더해 생성. 과적합을 줄일 수 있다. SMOTE도 많이 이용하지만 이 데이터에는 ROSE가 나을 것이라 판단했다. ROSE는 SMOTE보다 관측치에 덜 종속적이다. 데이터의 단순복제와 손실 문제가 따른다. SMOTE의 특징도 함께 적어 둔다. SMOTE Synthetic Minority Oversampling TEch. k-NN을 기반으로 소수범주 데이터를 선형결합해 훈련셋 생성. k-NN은 태생적으로 고차원 데이터에 취약하다. 그 외로도 ZIB(Zero-Inflated Binary), 오버샘플링, 언더샘플링 등의 접근법도 있지만 요즘은 거의 안 쓰이는 듯. 모델링기말 프로젝트라 그냥 할 수 있는 방법론은 다 때려넣어서 해 봤다. 결과적으로 보면 워스트 = BZIP, 베스트 = XGB라 좀 김이 빠지긴 한다. 학습, 하이퍼파라미터 튜닝 모두 10-fold CV를 함께 진행했다. Logistic Regression Saturated Model이 제일 나아서 그냥 그걸 썼는데, 역시 일별 플레이타임이 제일 유의했다. 성능은 그냥저냥. BZIP: Bivariate 영과잉 포아송 모델 참고 ROSE 없이 따로 훈련셋을 구성해 주어야 한다. R 기준 glmmADMB에서 간단히 활용할 수 있다. LDA 참고 LDA과 LR은 모두 선형적 decision boundaries를 갖는다. 이는 다차원 데이터에서도 마찬가지이다. 그러나 두 경우의 결과가 항상 같게 나타나지는 않는데, LDA는 공통분산행렬을 갖는 가우시안 분포를 가정하므로 이 가정이 맞지 않는 경우 성능이 떨어질 수 있다. QDA 참고 LR, LDA와 비모수추정법인 k-NN방식 간의 절충안으로 생각할 수 있다. Quadratic decision boundary를 가정하므로 보다 다양한 경우에 이용 가능하다. k-NN 차원의 저주 하면 kNN이다. Accuracy, AUC가 나이브 베이즈와 자강두천이었다. Random Forest 참고 Bootstrap 방식의 단점은 각 BS 샘플 간의 독립성이 확보된다고 가정하기 어렵다는 점에 있다. 랜덤포레스트 기법은 이를 보완하여, Bootstrap 샘플링 후 의사결정나무를 생성할 때 설명변수의 일부만을 사용한다. 조만간 이에 대해 정리한 글을 올릴 생각이다. (당연하지만) 부스팅을 제외한 모델들 중에선 제일 성능이 좋았다. Naive Bayes 참고 나이브 베이즈를 무시하지 말자. 간단한 베이지안 네트워크 모형이지만 적은 데이터에도 강한 성능을 보인다. (물론 각 변수 간의 독립성을 가정하기 때문에 독립성이 없는 모형에서는 그리 파워풀하지 못하다.) 학교 특성상 베이지안 예찬에 익숙한 걸 감안하더라도, 솔직히 차원의 저주에서 이만큼 자유로운 모형은 드물다. PCA만 잘 해도 일정 성능은 보장받는다니 얼마나 좋나 싶을 때가 많다. 물론 특출난 결과가 나오진 않았다. 특히 accuracy는 바닥을 쳤다… 그래도 90%는 넘었다…! SVM LGBM 참고 Gradient Boosting 방식의 프레임워크다. 즉, 트리 기반이다. 트리가 수직으로 확장된다는 점이 특징이다. 메모리가 적게 소비되고 빠르다. 정확도도 높다. 다만 과적합 위험이 따르므로 가급적 데이터가 클 때 쓰고, max depth가 너무 커지지 않도록 주의하도록 하자. AdaBoost 참고 심플한 약분류기(Weak Classifier)들이 상호보완하도록 순차학습하는 알고리즘이다. 한 개씩 순차학습하며, 먼저 학습된 분류기가 뱉은 오답 정보를 다음 분류기 학습에 적용한다. 이전 분류기의 오분류 샘플에 대한 가중치를 adaptive하게 알터하고 이들을 적절히 조합해 강분류기의 성능을 높인다. 성능은 XGB와 큰 차이 없이 준수하게 나타났다. XGBoost (Best) 모델 평가AUC, accuracy, precision-recall, F1 score를 고려하였다. 시각화같이 연구한 친구가 굉장히 멋진 그림을 그려왔었는데 [TN, FN, TP, FP]를 주요 Feature에 따라 나타낸 거였다. 각 캐릭터의 특징이 아주 잘 나타나서 정말 마음에 들었음. 내가 그린 게 아니라 올릴 수는 없다.. 단점Anomaly detection 쪽으로 갔으면 좋았을 것 같다고 생각한 이유가 이거다. 이렇게 특정 계정을 매크로라고 찍어내는 건 위양성에 대한 리스크가 크다.","categories":[],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"k-Means","slug":"006-k-means","date":"2020-11-01T02:52:39.000Z","updated":"2020-11-02T00:19:52.741Z","comments":true,"path":"2020/11/01/006-k-means/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/006-k-means/","excerpt":"","text":"k-Means Clustering각 클러스터와 거리 차이의 분산을 최소화하는 방식이다. EM식 클러스터링과 유사한 면이 있다. 알고리즘 개요 Input: k = Number of clusters to be used x, y = two numeric vectors identical in lengths x와 y의 random cluster centers 생성:x.centers ~ Unif( min(x), max(x) )y.centers ~ Unif( min(y), max(y) ) i = 1, 2, … , length(x)일 때x[i]와 x.centers, y[i]와 y.centers 간의 거리를 통해 유클리드 거리 측정개중 가장 거리가 가깝게 나타나는 x.center, y.center 쌍에 해당하는 cluster를 해당 점에 매칭 subset(x, cluster)의 평균 계산 후 이 값과 기존의 x.center, y.center이 동일하면 loop 중단 빠르다. k-Median, k-Means++, k-Medoid 등의 변형이 있다. Elbow Method: within-cluster sum of squares를 line chart로 나타내어 눈에 띄게 꺾이는 부분을 클러스터 개수로 설정하는 경우가 많다. x, y에 각각 N(10, 4), N(-10, 2)를 따르는 50개의 난수를 부여하고, 이를 클러스터링하였다. k값에 따른 클러스터의 분포는 다음과 같이 나타난다. Partitioning Around Medoids (PAM) k-Medoid 클러스터링 중 가장 널리 쓰이는 알고리즘이다. 노이즈와 이상치에 보다 Robust하다. 해석이 용이하다. Greedy Search: 최적치에 도달하지 못할 수 있다. 그래도 Exhaustive Search보단 훨씬 빠르다. 알고리즘 개요 Initialize: greedily select k of the n data points as the medoids to minimize the cost Associate each data point to the closest medoid. While the cost of the configuration decreases: For each medoid m, and for each non-medoid data point o: Consider the swap of m and o, and compute the cost change If the cost change is the current best, remember this m and o combination Perform the best swap of $m_{best}$and $o_{best}$, if it decreases the cost function. Otherwise, the algorithm terminates. 대푯값(medoid)을 도출하는 데에는 Gower’s Distance를 활용할 수 있다.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Optimization","slug":"005-optimization","date":"2020-11-01T02:50:59.000Z","updated":"2020-11-02T00:19:45.343Z","comments":true,"path":"2020/11/01/005-optimization/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/005-optimization/","excerpt":"","text":"local maxima/minima를 찾는 고전적 알고리즘 세 가지를 알아보자. Bisection Method도함수가 0이 되는 지점을 찾기 위해 대상 구간을 반으로 자른다. 잘린 경계값에서의 도함수 값을 곱해 음수가 되는 부분을 찾는다. 음수인 부분을 또 자른다. 이렇게 반복해서 반절(半折)하다 보면 0에 수렴하는 지점을 찾을 수 있다. 123456789101112131415161718192021bisec.ftn = function(g,a,b)&#123; iter = 0 threshold = 10^(-9) x.new = (a+b)/2 err = 1 while(iter&lt; 3000 &amp; err&gt;=threshold)&#123; z.a = genD(g,a) z.x = genD(g,x.new) if(z.a$D[1] * z.x$D[1] &lt; 0) b = x.new else a = x.new old.x = x.new x.new = (a+b)/2 err = abs(old.x - x.new) iter = iter + 1 &#125; return(list(MLE = x.new, iterations = iter))&#125; Newton’s Method (Newton-Rhapson)테일러 전개된 수식을 통해 $x_{t+1}$ 을 업데이트한다. 직관적이고 간편하지만 초기값을 잘못 설정하면 수렴이 불가능할 수 있으며, 이차 도함수를 모르면 사용할 수 없다. 1234567891011121314151617newton.ftn = function(g, theta)&#123; threshold = 10^(-9) err = 1 theta_0 = theta iter = 1 while(err &gt;= threshold &amp; iter &lt; 3000)&#123; z = genD(g, theta_0) theta.new = theta_0 - z$D[1]/z$D[2] err = abs(theta.new - theta_0) theta_0 = theta.new iter = iter + 1 &#125; return(list(MLE = theta.new, iterations = iter))&#125; Secant MethodNewton’s Method의 파생 알고리즘이다. 두 개의 초기값을 설정해 계산한다. 발산 문제에서 자유로우며, 이차 도함수를 알지 못해도 적용할 수 있다. 1234567891011121314151617181920secant.ftn = function(g,a,b)&#123; iter = 1 threshold = 10^(-9) err = 1 x0 = a x1 = b while(err &gt;= threshold &amp; iter &lt; 3000)&#123; z0 = genD(g, x0) z1 = genD(g, x1) x.new = x1 - z1$D[1]*(x1 - x0) / (z1$D[1] - z0$D[1]) err = abs(x.new - x1) x0 = x1 x1 = x.new iter = iter + 1 &#125; return(list(MLE = x.new, iterations = iter))&#125; $Cauchy(\\theta, 1)$ 분포에서 추출한 샘플을 통해 MLE를 구한다면, 각 method는 어떻게 작동할까? 위와 같은 로그우도함수에 대해 각 방식은 아래와 같은 결과를 보였다. Bisection Starting Points Estimated Iterations (-1, 1) -0.1922866 34 (-100, 100) -0.1922866 40 (-10^6, -10^6) -0.1922866 54 (-1, 3) -0.1922866 35 (30, 40) 40 36 Global Maximum이 없는 구간에서 시작하면 MLE를 찾을 수 없다. Newton-Rhapson Starting Points Estimated Iterations -11 - - -1 - 0.1922866 7 0 - 0.1922866 5 1.5 1.713587 6 4 2.817472 7 4.7 - 0.1922866 7 7 41.04085 11 8 - - 38 42.79538 7 5.689 54.87662 10 Secant Starting Points Estimated Iterations (-2, -1) -0.1922866 8 (-3, 3) 2.817472 9 (-10^6, -10^6) - - (-1, 3) 1.713587 12 (30, 40) 42.79538 13 좀 더 복잡한 분포를 살펴보자. f (x) = \\frac{[1 − cos(x − \\theta)]}{2π}0 ≤ x ≤ 2π, where θ is a parameter between −π and π 위와 같은 로그우도함수에 대해 Newton-Rhapson 방식을 적용하자. 취약점이 보일 것이다. $\\theta$ 의 method-of-moments estimator은 $\\hat \\theta = arcsin(\\pi - \\hat \\mu) = -0.05844$ 이므로 이 부근에서 시작한 경우와 랜덤한 두 점 (-2.7과 2.7)에서 시작한 경우를 비교해 보았다. Newton-Rhapson Starting Points Estimated Iterations -0.0584 -0.01197 7 -2.7 -2.6667 6 2.7 2.8731 7 서로 판이하게 다른 값이 도출된다. MoM 추정치에서 시작한 값만이 정답에 수렴하고 있다. 동일한 크기로 슬라이스한 200개의 구간에서 각각 결과를 구해 보았다. 앞의 19개에서는 이러한 아웃풋이 나오는데, Group 11 (-0.81681, 0.52027) 에서 또 정답이 보인다. Group Lower Upper Estimated 1 -3.14159 -2.82743 -3.09309 2 -2.79602 -2.7646 -2.78617 3 -2.73319 -2.60752 -2.6667 4 -2.57611 -2.41903 -2.50761 5 -2.38761 -2.38761 -2.3882 6 -2.35619 -2.26195 -2.29726 7 -2.23053 -2.23053 -2.23217 8 -2.19911 -1.47655 -1.65828 9 -1.44513 -1.44513 -1.44748 10 -1.41372 -0.84823 -0.95334 11 -0.81681 0.502655 -0.01197 12 0.534071 1.947787 0.7906 13 1.979203 2.199115 2.00364 14 2.230531 2.261947 2.23622 15 2.293363 2.450442 2.36072 16 2.481858 2.481858 2.47537 17 2.513274 2.513274 2.51359 18 2.54469 2.984513 2.87309 19 3.0159289 3.1101767 3.19009 서로 아주 가까운 두 값을 시작값으로 상정한다면 이러한 결과를 얻게 된다. Lower Upper Estimated -1.41372 -0.82377 -0.95334 -0.82209 0.403176 -0.01197 0.404857 0.404857 8.28683 0.406538 0.492256 -0.01197 0.493937 0.493937 0.7906 0.495618 0.519148 -0.01197 0.520829 1.946107 0.7906 요약 Bisection Method는 일정 구간의 경계값에서 미분값을 곱하여 음수가 되는 곳을 따라 진행하므로, 구간의 길이가 길수록 연산 시간도 길어지게 되며, 연산의 효율성이 떨어진다. 대신 발산 문제에서 자유로운 편이다. Newton’s Method는 초기값에 영향을 크게 받으며, 적절치 못한 값으로 연산을 시작하게 된다면 정확하지 않은 결과를 얻게 될 가능성이 크다. Secant Method는 이차 도함수를 얻기 어려운 경우, Bisection Method보다 빠르게 값을 도출할 수 있는 방법이라 할 수 있다.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"차원 축소","slug":"008-t-SNE","date":"2020-11-01T02:18:18.000Z","updated":"2020-11-02T00:21:21.001Z","comments":true,"path":"2020/11/01/008-t-SNE/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/008-t-SNE/","excerpt":"","text":"PCAt-SNE 데이터의 공분산을 분석하여 선형변환 행렬(linear transformation matriix)을 얻는다. PCA는 선형변환을 이용하기 때문에 비선형 특성을 가진 데이터의 특성을 잘 추출하지 못한다. 기존의 방식대로 KL-divergence로 similarity를 계산하는 것은 symetric하지 않다. Sillhouette Width 클러스터링 품질 점수 $x_i$의 실루엣 계수: $a(i)$: 클러스터 내 데이터 응집도 (cohesion) $b(i)$: 클러스터 간 분리도 (separation) 1에 가까울수록 좋다","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Stepwise Selection과 Regsubset","slug":"007-step-regsubset","date":"2020-11-01T01:08:18.000Z","updated":"2020-11-02T00:21:08.517Z","comments":true,"path":"2020/11/01/007-step-regsubset/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/007-step-regsubset/","excerpt":"","text":"회귀모형에서의 변수 선택을 간단히 다루어 보자. 변수 선택하기 - Stepwise Selection, regsubsetsbaseball 데이터에는 야수 337명에 대한 27가지의 스탯과 연봉이 나타나 있다. 스탯은 1991년도 기준이고, 연봉은 1992년도에 책정된 값이다. (데이터) 스탯의 변수명 average OBP runs hits doubles triples homeruns 타율 출루율 득점 안타 2루타 3루타 홈런 SBs RBIs walks SOs errors freeagent arbitration 도루 타점 볼넷 삼진 실책 FA 연봉조정 runs/SOs, hits/SOs , RBIs/SOs , walks/SOs OBP/errors, runs/errors , hits/errors, homeruns/errors SOs*errors, SBs*OBP, SBs*runs, SBs*hits 연봉을 반응변수로 삼아 회귀 모형을 만들어 보자. 이를 Full Model, 또는 Saturated Model이라 한다. 다중 회귀 모형에는 관행적으로 변수 선택을 진행한다. 줄여진 모형은 Reduced Model이라 한다. AIC를 기준으로 변수를 줄이는 방법을 두 가지 살펴보겠다. Stepwise SelectionStepwise Selection 알고리즘은 변수를 하나씩 제거하거나 추가하면서 각 경우의 AIC를 비교하고, 개중 가장 낮은 값을 갖는 모형을 찾는다. Backward Selection: 모든 변수를 가진 saturated model에서 변수를 하나씩 소거해 나간다. 제거 후에 AIC가 작아졌다면 제거된 모형을 채택하고 더 이상 AIC가 나아지지 않을 때까지 같은 작업을 반복한다. Forward Selection: 반대로 상수만 지닌 reduced model에서 변수를 하나씩 추가하는 방식이다. “Both”: 두 가지를 절충하여 추가와 제거를 반복하고자 하는 경우, (R 기준) both 인자를 넣는다. 각 방식을 적용한 결과는 다음과 같이 나타났다. 채택 변수 수 최종 AIC Backward 12 -418.9421 Forward 27 -396.706 Both 12 -418.9421 Forward는 전체 설명변수를 모두 가져왔다. Both와 Backward에서는 동일한 모형이 채택되었다. 이 때 선택된 변수는 다음과 같다: 타율(-), 득점(+), 삼루타(-), 타점(+), 삼진(-), FA(+), 연봉조정(+), 득점/삼진(-), 안타/삼진(+), 삼진x실책(-), 도루x출루율(+), 도루x득점(-). regsubsetsregsubsets 알고리즘은 고정된 변수 개수에 맞추어 최적 모형을 도출한다. 12345678910111213141516171819regsub.AIC.ftn = function(j)&#123; model.row = regsub.baseball$which[j,] model.exp = &#x27;lm( log(salary) ~ &#x27; for (i in 2:28)&#123; if (model.row[i] == TRUE)&#123; var.name = names(model.row)[i] model.exp = paste(model.exp, &quot; + &quot;, var.name) &#125; &#125; model.exp = paste(model.exp,&quot;, data = df.baseball)&quot;) result = eval(parse(text = model.exp)) if(j &lt; 14)&#123; varnames = paste(names(model.row)[model.row], collapse = &#x27; &#x27;) &#125; else varnames = paste(&#x27;-&#x27;, names(model.row)[!model.row], collapse = &#x27; &#x27;) return(c(AIC = extractAIC(result)[2], var = varnames))&#125; p = 1에서 27까지의 최적 Subsets는 아래의 표에 나타내었다. 개중 AIC가 가장 작은 모형은 p = 12에서 얻을 수 있었다. p AIC Features Selected 1 -86.20355 hits 2 -236.91899 freeagent arbitration 3 -382.08585 rbis freeagent arbitration 4 -397.06152 hits rbis freeagent arbitration 5 -406.00412 hits rbis freeagent arbitration soserrors 6 -410.43729 runs rbis sos freeagent arbitration runsperso 7 -413.72379 runs rbis sos freeagent arbitration runsperso hitsperso 8 -416.30347 runs rbis sos freeagent arbitration obppererror runspererror hitspererror 9 -416.42493 runs rbis sos freeagent arbitration obppererror runspererror hitspererror soserrors 10 -417.47395 runs rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 11 -418.45106 average runs rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 12 -418.94721 obp runs triples rbis sos freeagent arbitration runsperso hitsperso soserrors sbsobp sbsruns 13 -418.09221 average runs triples rbis sos freeagent arbitration runsperso hitsperso hrsperso soserrors sbsobp sbsruns 14 -417.20098 - average - hits - doubles - homeruns - sbs - errors - rbisperso - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 15 -416.22752 - average - hits - doubles - homeruns - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 16 -415.45316 - average - doubles - walks - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 17 -414.01172 - average - doubles - sbs - errors - walksperso - obppererror - runspererror - hitspererror - hrspererror - sbshits 18 -413.16254 - average - doubles - walks - sbs - walksperso - runspererror - hrspererror - soserrors - sbshits 19 -411.53320 - average - doubles - sbs - walksperso - runspererror - hrspererror - soserrors - sbshits 20 -410.00084 - obp - doubles - sbs - runspererror - hrspererror - soserrors - sbshits 21 -408.22846 - obp - doubles - sbs - errors - hrspererror - sbshits 22 -406.42133 - obp - sbs - errors - hrspererror - sbshits 23 -404.56810 - obp - sbs - errors - sbshits 24 -402.63184 - obp - errors - sbshits 25 -400.66945 - errors - sbshits 26 -398.69257 - sbshits 27 -396.70603 - 최적의 모형인 p = 12에서 선택된 변수는 출루율(-), 득점(+), 삼루타(-), 타점(+), FA(+), 연봉조정(+), 득점삼진비(-), 안타삼진비(+), 도루x실책(-), 도루x출루율(+), 도루x득점(-)이다. 양쪽 결과가 비슷하게 나타났다. 문장으로 풀어 쓴다면 안타가 많고 타율이 좋은 타자는 오히려 낮은 연봉을 받는다고 해석할 수 있다. 이는 높은 연봉이 책정되는 홈런 타자들에게 영향을 받은 결과로 보인다. (3루타의 경우, 장타력뿐만 아니라 상당한 주력이 함께 뒷받침되어야 하는 데다 부상의 위험이 따르므로 대부분의 거포형 선수와 베테랑 선수들은 2루에서 멈추게 된다.) 큰 폭으로 연봉이 상승하게 되는 FA 전환 여부 또한 모형에 주는 영향이 크다. 연봉 조정 역시 같은 맥락에서 해석할 수 있을 것이다. * ScopeR에서 step함수를 사용한다면 변수의 범위를 지정할 수 있다. 123step(model, direction = [&#x27;forward&#x27;,&#x27;backward&#x27;], scope = list(upper = y ~ ., lower = y ~ x1 + x2 + x3 + x4 + x5))","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"004-lift","slug":"004-lift","date":"2020-11-01T00:04:48.000Z","updated":"2020-11-02T00:19:35.422Z","comments":true,"path":"2020/11/01/004-lift/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/004-lift/","excerpt":"","text":"","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[]},{"title":"Reducting Feature Dimensions","slug":"003-curse-of-dimensionality","date":"2020-10-31T23:01:12.000Z","updated":"2020-11-02T00:19:30.136Z","comments":true,"path":"2020/11/01/003-curse-of-dimensionality/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/003-curse-of-dimensionality/","excerpt":"","text":"차원 축소하기Curse of Dimensionality: 차원의 저주간단히 말해 고차원 데이터를 경계해야 한다는 뜻이다. 고차원 공간은 엄청나게 넓기 때문에 공간 속에 있는 데이터 포인트는 매우 외로운 나날을 보내게 된다. (near neighbor의 수가 적다고 표현한다.) n개의 points를 갖는 random sample에 대해 kernel density estimator을 정의하자. (이 때, 해당 데이터는 p차원의 표준정규분포를 따른다고 가정한다.) 굳이 알 필요는 없지만, 진짜 mode에 대한 다변량 density estomator의 퀄리티를 판단하는 기준은 ORRMSE(Optimal Relative Root MSE)이다. ORRMSE의 식은 다음과 같다. ORRMSE(p,n) = \\frac{\\sqrt{min_h[MSE_h(\\hat f(0))]}}{f(0)}우리의 목표는 ORRMSE를 작게 유지하는 것이다. 차원 p가 올라가면 적정 샘플 사이즈 n은 그보다 훨씬 큰 폭으로 증가한다. Computational Statistics (Givens and Hoeting, Wiley) 에 제시된 예시에 대한 결과표를 보자. p n 1 30 2 180 3 806 5 17,400 10 112,000,000 15 2,190,000,000,000 30 806,000,000,000,000,000,000,000,000 변수가 수백 개인 데이터도 드물지 않은데, 그 때마다 어마어마한 양의 샘플을 준비하는 것은 현실적으로 불가능하다. 그렇다면 해결책은 하나 뿐이다. 차원을 줄이는 것이다.","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Collaborative Filtering","slug":"002-CF","date":"2020-10-31T22:57:54.000Z","updated":"2020-11-02T00:19:23.719Z","comments":true,"path":"2020/11/01/002-CF/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/002-CF/","excerpt":"","text":"협업 필터링User Based CF유저 기반 CF는 사용자 간 유사도를 기준으로 삼는다. Item Based CF","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"}]},{"title":"Eigenvalue와 Eigenvectors","slug":"001-eigen","date":"2020-10-31T17:57:25.000Z","updated":"2020-11-02T00:19:15.410Z","comments":true,"path":"2020/11/01/001-eigen/","link":"","permalink":"http://ioio-noo.github.io/2020/11/01/001-eigen/","excerpt":"","text":"Eigenvalue, Eigenvectors 위와 같이, 임의의 정방행렬 A에 대해 AB = λB인 (단, B는 0이 아닌 벡터) B를 eigenvector, 상수 λ를 eigenvalue 라고 한다. Eigen Decomposition Av = \\lambda v \\\\ AQ = Q\\Lambda \\\\ A = Q\\Lambda Q^{-1}","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Eigenvalues, Eigenvectors, SVD","slug":"Eigenvalues-Eigenvectors-SVD","permalink":"http://ioio-noo.github.io/tags/Eigenvalues-Eigenvectors-SVD/"}]}],"categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/categories/Data-Science/"}],"tags":[{"name":"Data Science","slug":"Data-Science","permalink":"http://ioio-noo.github.io/tags/Data-Science/"},{"name":"Eigenvalues, Eigenvectors, SVD","slug":"Eigenvalues-Eigenvectors-SVD","permalink":"http://ioio-noo.github.io/tags/Eigenvalues-Eigenvectors-SVD/"}]}